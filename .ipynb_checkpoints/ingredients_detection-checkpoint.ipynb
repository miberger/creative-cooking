{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingredients: Download and Detection\n",
    "\n",
    "by Michael Berger\n",
    "\n",
    "**Sources:**\n",
    "- Horea Muresan, Mihai Oltean, Fruit recognition from images using deep learning, Acta Univ. Sapientiae, Informatica Vol. 10, Issue 1, pp. 26-42, 2018.\n",
    "- Scraping Google Images\n",
    "- https://github.com/marcusklasson/GroceryStoreDataset\n",
    "- https://github.com/PhilJd/freiburg_groceries_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, Activation, Dropout, Lambda\n",
    "import tensorflow as tf\n",
    "from PIL import ImageFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allowing to load truncated images\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading grocery dataset\n",
    "# http://ais.informatik.uni-freiburg.de/publications/papers/jund16groceries.pdf\n",
    "# https://github.com/PhilJd/freiburg_groceries_dataset\n",
    "\n",
    "from subprocess import call\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "dataset_url = \"http://aisdatasets.informatik.uni-freiburg.de/\" \\\n",
    "              \"freiburg_groceries_dataset/freiburg_groceries_dataset.tar.gz\"\n",
    "\n",
    "print(\"Downloading dataset.\")\n",
    "urllib.request.urlretrieve(dataset_url, \"/home/miber/data/freiburg_groceries_dataset.tar.gz\")\n",
    "print(\"Extracting dataset.\")\n",
    "call([\"tar\", \"-xf\", \"/home/miber/data/freiburg_groceries_dataset.tar.gz\", \"-C\", \"/home/miber/data/\"])\n",
    "os.remove(\"/home/miber/data/freiburg_groceries_dataset.tar.gz\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset.\n",
      "Extracting dataset.\n",
      "Done.\n",
      "Downloading dataset.\n",
      "Extracting dataset.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Grocery image dataset\n",
    "# https://github.com/gulvarol/grocerydataset\n",
    "\n",
    "# NOT USEFUL, CONTAINS CIGARETTES ONLY!!!!\n",
    "\n",
    "# Part 1\n",
    "\n",
    "dataset_url = \"https://github.com/gulvarol/grocerydataset/releases/download/1.0/GroceryDataset_part1.tar.gz\"\n",
    "\n",
    "print(\"Downloading dataset.\")\n",
    "urllib.request.urlretrieve(dataset_url, \"/home/miber/data/GroceryDataset_part1.tar.gz\")\n",
    "print(\"Extracting dataset.\")\n",
    "call([\"tar\", \"-xf\", \"/home/miber/data/GroceryDataset_part1.tar.gz\", \"-C\", \"/home/miber/data/\"])\n",
    "os.remove(\"/home/miber/data/GroceryDataset_part1.tar.gz\")\n",
    "print(\"Done.\")\n",
    "\n",
    "# Part 2\n",
    "\n",
    "dataset_url = \"https://github.com/gulvarol/grocerydataset/releases/download/1.0/GroceryDataset_part2.tar.gz\"\n",
    "\n",
    "print(\"Downloading dataset.\")\n",
    "urllib.request.urlretrieve(dataset_url, \"/home/miber/data/GroceryDataset_part2.tar.gz\")\n",
    "print(\"Extracting dataset.\")\n",
    "call([\"tar\", \"-xf\", \"/home/miber/data/GroceryDataset_part2.tar.gz\", \"-C\", \"/home/miber/data/\"])\n",
    "os.remove(\"/home/miber/data/GroceryDataset_part2.tar.gz\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramaters for learning the ingredients detection mode\n",
    "\n",
    "# NN parameters\n",
    "epochs = 1\n",
    "batch_size = 25\n",
    "validation_percent = 0.15\n",
    "learning_rate = 0.1 # initial learning rate\n",
    "min_learning_rate = 0.00001  # threshold to stop reducing learning rate\n",
    "learning_rate_reduction_factor = 0.5  # learning_rate *= learning_rate_reduction_factor\n",
    "patience = 3  # how many epochs to wait before reducing the learning rate when the loss plateaus\n",
    "verbose = 1  # controls the amount of logging done during training and testing: 0 - none, 1 - reports metrics after each batch, 2 - reports metrics after each epoch\n",
    "image_size = (100, 100)  # width and height of the used images\n",
    "input_shape = (100, 100, 3)\n",
    "weight = 0.01 #L2 weight\n",
    "\n",
    "# Input and output parameters\n",
    "use_label_file = True  # set this to true if you want load the label names from a file; uses the label_file defined below; the file should contain the names of the used labels, each label on a separate line\n",
    "label_file = 'labels.txt'\n",
    "base_dir = '/home/miber/' # for data and output files\n",
    "train_dir = os.path.join(base_dir, 'data/ingredients-images/ingredients-train')\n",
    "test_dir = os.path.join(base_dir, 'data/ingredients-images/ingredients-test')\n",
    "output_dir = 'ingredients_training_output_files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Walnut', 'Orange', 'Nut Pecan', 'Onion White', 'Honey', 'Romanesco', 'Pomelo', 'Dates', 'Mulberry', 'Cantaloupe', 'Pitahaya', 'Melon Piel de Sapo', 'Mushroom', 'Cocos', 'Rice', 'Eggplant', 'Pasta', 'Tomato', 'Quince', 'Pineapple', 'Grapefruit', 'Vinegar', 'Yoghurt', 'Plum', 'Asparagus', 'Hazelnut', 'Cucumber', 'Soy Milk', 'Pomegranate', 'Rambutan', 'Ginger Root', 'Maracuja', 'Lemon', 'Apple', 'Granadilla', 'Peach', 'Mangostan', 'Flour', 'Cactus Fruit', 'Guava', 'Beans', 'Bell Pepper', 'Strawberry', 'Leek', 'Kiwi', 'Watermelon', 'Sugar', 'Physalis', 'Passion Fruit', 'Corn', 'Kohlrabi', 'Bok Choi', 'Apricot', 'Cherry', 'Cabbage', 'Blueberries', 'Kumquats', 'Cauliflower', 'Garlic', 'Huckleberry', 'Tamarillo', 'Tomato Sauce', 'Lime', 'Grape', 'Oil', 'Sweet Potato', 'Lychee', 'Raspberry', 'Pear', 'Sour Cream', 'Potato', 'Eggs', 'Redcurrant', 'Papaya', 'Banana Red', 'Milk', 'Celery', 'Carambula', 'Oat Milk', 'Avocado', 'Mandarin Orange', 'Mango', 'Pepino', 'Chestnut', 'Kaki', 'Nectarine', 'Zucchini', 'Clementine', 'Beetroot', 'Onion Red', 'Salak', 'Carrots', 'Nut Forest', 'Banana', 'Tangelo']\n",
      "95\n"
     ]
    }
   ],
   "source": [
    "# write label file\n",
    "labels = os.listdir(train_dir)\n",
    "\n",
    "with open(label_file, \"w\") as f:\n",
    "    for label in labels:\n",
    "        f.writelines(label + '\\n')\n",
    "\n",
    "print(labels)\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Walnut', 'Orange', 'Nut Pecan', 'Onion White', 'Honey', 'Romanesco', 'Pomelo', 'Dates', 'Mulberry', 'Cantaloupe', 'Pitahaya', 'Melon Piel de Sapo', 'Mushroom', 'Cocos', 'Rice', 'Eggplant', 'Pasta', 'Tomato', 'Quince', 'Pineapple', 'Grapefruit', 'Vinegar', 'Yoghurt', 'Plum', 'Asparagus', 'Hazelnut', 'Cucumber', 'Soy Milk', 'Pomegranate', 'Rambutan', 'Ginger Root', 'Maracuja', 'Lemon', 'Apple', 'Granadilla', 'Peach', 'Mangostan', 'Flour', 'Cactus Fruit', 'Guava', 'Beans', 'Bell Pepper', 'Strawberry', 'Leek', 'Kiwi', 'Watermelon', 'Sugar', 'Physalis', 'Passion Fruit', 'Corn', 'Kohlrabi', 'Bok Choi', 'Apricot', 'Cherry', 'Cabbage', 'Blueberries', 'Kumquats', 'Cauliflower', 'Garlic', 'Huckleberry', 'Tamarillo', 'Tomato Sauce', 'Lime', 'Grape', 'Oil', 'Sweet Potato', 'Lychee', 'Raspberry', 'Pear', 'Sour Cream', 'Potato', 'Eggs', 'Redcurrant', 'Papaya', 'Banana Red', 'Milk', 'Celery', 'Carambula', 'Oat Milk', 'Avocado', 'Mandarin Orange', 'Mango', 'Pepino', 'Chestnut', 'Kaki', 'Nectarine', 'Zucchini', 'Clementine', 'Beetroot', 'Onion Red', 'Salak', 'Carrots', 'Nut Forest', 'Banana', 'Tangelo']\n",
      "95\n"
     ]
    }
   ],
   "source": [
    "# Ensure that test labels are the same\n",
    "test_labels = os.listdir(test_dir)\n",
    "    \n",
    "test_num_classes = len(test_labels)\n",
    "\n",
    "print(test_labels)\n",
    "print(test_num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Walnut', 'Orange', 'Nut Pecan', 'Onion White', 'Honey', 'Romanesco', 'Pomelo', 'Dates', 'Mulberry', 'Cantaloupe', 'Pitahaya', 'Melon Piel de Sapo', 'Mushroom', 'Cocos', 'Rice', 'Eggplant', 'Pasta', 'Tomato', 'Quince', 'Pineapple', 'Grapefruit', 'Vinegar', 'Yoghurt', 'Plum', 'Asparagus', 'Hazelnut', 'Cucumber', 'Soy Milk', 'Pomegranate', 'Rambutan', 'Ginger Root', 'Maracuja', 'Lemon', 'Apple', 'Granadilla', 'Peach', 'Mangostan', 'Flour', 'Cactus Fruit', 'Guava', 'Beans', 'Bell Pepper', 'Strawberry', 'Leek', 'Kiwi', 'Watermelon', 'Sugar', 'Physalis', 'Passion Fruit', 'Corn', 'Kohlrabi', 'Bok Choi', 'Apricot', 'Cherry', 'Cabbage', 'Blueberries', 'Kumquats', 'Cauliflower', 'Garlic', 'Huckleberry', 'Tamarillo', 'Tomato Sauce', 'Lime', 'Grape', 'Oil', 'Sweet Potato', 'Lychee', 'Raspberry', 'Pear', 'Sour Cream', 'Potato', 'Eggs', 'Redcurrant', 'Papaya', 'Banana Red', 'Milk', 'Celery', 'Carambula', 'Oat Milk', 'Avocado', 'Mandarin Orange', 'Mango', 'Pepino', 'Chestnut', 'Kaki', 'Nectarine', 'Zucchini', 'Clementine', 'Beetroot', 'Onion Red', 'Salak', 'Carrots', 'Nut Forest', 'Banana', 'Tangelo']\n",
      "95\n"
     ]
    }
   ],
   "source": [
    "# Set up for model\n",
    "# Based on https://github.com/Horea94/Fruit-Images-Dataset\n",
    "\n",
    "# Setting up output folder\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Loading labels\n",
    "if use_label_file:\n",
    "    with open(label_file, \"r\") as f:\n",
    "        labels = [x.rstrip('\\n') for x in f.readlines()]\n",
    "else:\n",
    "    labels = os.listdir(train_dir)\n",
    "    \n",
    "num_classes = len(labels)\n",
    "\n",
    "print(labels)\n",
    "print(num_classes)\n",
    "\n",
    "# Functions for plotting, data generator and model evaluation\n",
    "def plot_model_history(model_history, out_path=\"\"):\n",
    "    '''\n",
    "    Creating one accuracy and one loss chart over epochs\n",
    "    '''\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # summarize history for accuracy\n",
    "    axs[0].plot(range(1, len(model_history.history['accuracy']) + 1), model_history.history['accuracy'])\n",
    "    axs[0].plot(range(1, len(model_history.history['val_accuracy']) + 1), model_history.history['val_accuracy'])\n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_xticks(np.arange(1, len(model_history.history['accuracy']) + 1), len(model_history.history['accuracy']))\n",
    "    axs[0].legend(['train', 'val'], loc='best')\n",
    "    \n",
    "    # summarize history for loss\n",
    "    axs[1].plot(range(1, len(model_history.history['loss']) + 1), model_history.history['loss'])\n",
    "    axs[1].plot(range(1, len(model_history.history['val_loss']) + 1), model_history.history['val_loss'])\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_xticks(np.arange(1, len(model_history.history['loss']) + 1), len(model_history.history['loss']))\n",
    "    axs[1].legend(['train', 'val'], loc='best')\n",
    "    \n",
    "    # save the graph in a file called \"acc_loss.png\" to be available for later\n",
    "    # the model_name is provided when creating and training a model\n",
    "    if out_path:\n",
    "        plt.savefig(out_path + \"/acc_loss.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, out_path=\"\"):\n",
    "    '''\n",
    "    Creates a confusion matrix to visually represent incorrectly classified images\n",
    "    '''\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    df_cm = pd.DataFrame(cm, index=[i for i in classes], columns=[i for i in classes])\n",
    "    plt.figure(figsize=(100, 100))\n",
    "    ax = sn.heatmap(df_cm, annot=True, square=True, fmt=\"d\", linewidths=.2, cbar_kws={\"shrink\": 0.8})\n",
    "    \n",
    "    if out_path:\n",
    "        plt.savefig(out_path + \"/confusion_matrix.png\")  # as in the plot_model_history, the matrix is saved in a file called \"model_name_confusion_matrix.png\"\n",
    "    \n",
    "    return ax\n",
    "\n",
    "\n",
    "# builds a training and validation generator\n",
    "def build_data_generators(train_folder, test_folder, validation_percent=0.2, labels=None, \n",
    "                          image_size=image_size, batch_size=50):\n",
    "    '''\n",
    "    Builds training and validation generator from training dataset\n",
    "    '''\n",
    "    \n",
    "    # It should randomly flip images\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        width_shift_range=0.0,\n",
    "        height_shift_range=0.0,\n",
    "        zoom_range=0.0,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,  \n",
    "        validation_split=validation_percent)\n",
    "\n",
    "    train_gen = train_datagen.flow_from_directory(train_folder, target_size=image_size, class_mode='sparse',\n",
    "                                                  batch_size=batch_size, shuffle=True, \n",
    "                                                  subset='training', classes=labels)\n",
    "    \n",
    "    validation_gen = train_datagen.flow_from_directory(train_folder, target_size=image_size, class_mode='sparse',\n",
    "                                                       batch_size=batch_size, shuffle=False, \n",
    "                                                       subset='validation', classes=labels)\n",
    "    test_datagen = ImageDataGenerator(\n",
    "        width_shift_range=0.0,\n",
    "        height_shift_range=0.0,\n",
    "        zoom_range=0.0,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True)\n",
    "    \n",
    "    test_gen = test_datagen.flow_from_directory(test_folder, target_size=image_size, class_mode='sparse',\n",
    "                                                  batch_size=batch_size, shuffle=False, classes=labels)\n",
    "    \n",
    "    return train_gen, validation_gen, test_gen\n",
    "\n",
    "\n",
    "def train_and_evaluate_model(model, name=\"\", epochs=epochs, batch_size=batch_size, \n",
    "                             validation_percent=validation_percent, verbose=verbose, \n",
    "                             loss='categorical_hinge', use_checkpoint=False):\n",
    "    '''\n",
    "    Training and evaluating the model\n",
    "    \n",
    "    The batch size is used to determine the number of images passed through the network at once, \n",
    "    the number of steps per epochs is derived from this as (total number of images in set // batch size) + 1\n",
    "    '''\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    model_out_dir = os.path.join(output_dir, name)\n",
    "    \n",
    "    if not os.path.exists(model_out_dir):\n",
    "        os.makedirs(model_out_dir)\n",
    "        \n",
    "    if use_checkpoint:\n",
    "        model.load_weights(model_out_dir + \"/model.h5\")\n",
    "\n",
    "    trainGen, validationGen, testGen = build_data_generators(train_dir, test_dir, validation_percent=validation_percent, \n",
    "                                                    labels=labels, image_size=image_size, batch_size=batch_size)\n",
    "    \n",
    "    print('Training directory:\\n', train_dir)\n",
    "    \n",
    "    optimizer = Adadelta(lr=learning_rate)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', patience=patience, verbose=verbose, \n",
    "                                                factor=learning_rate_reduction_factor, min_lr=min_learning_rate)\n",
    "    \n",
    "    save_model = ModelCheckpoint(filepath=model_out_dir + \"/model.h5\", monitor='val_accuracy', verbose=verbose, \n",
    "                                 save_best_only=True, save_weights_only=False, mode='max', period=1)\n",
    "    \n",
    "    history = model.fit_generator(generator=trainGen,\n",
    "                                  epochs=epochs,\n",
    "                                  steps_per_epoch=(trainGen.n // batch_size) + 1,\n",
    "                                  validation_data=validationGen,\n",
    "                                  validation_steps=(validationGen.n // batch_size) + 1,\n",
    "                                  verbose=verbose,\n",
    "                                  callbacks=[learning_rate_reduction, save_model])\n",
    "\n",
    "    model.load_weights(model_out_dir + \"/model.h5\")\n",
    "\n",
    "    validationGen.reset()\n",
    "    loss_v, accuracy_v = model.evaluate_generator(validationGen, steps=(validationGen.n // batch_size) + 1, verbose=verbose)\n",
    "    \n",
    "    print(\"Validation: accuracy = %f  ;  loss_v = %f\" % (accuracy_v, loss_v))\n",
    "    \n",
    "    loss_t, accuracy_t = model.evaluate_generator(testGen, steps=(testGen.n // batch_size) + 1, verbose=verbose)\n",
    "    \n",
    "    print(\"Hold out set: accuracy = %f  ;  loss = %f\" % (loss_t, accuracy_t))\n",
    "    \n",
    "    plot_model_history(history, out_path=model_out_dir)\n",
    "    \n",
    "    testGen.reset()\n",
    "    y_pred = model.predict_generator(testGen, steps=(testGen.n // batch_size) + 1, verbose=verbose)\n",
    "    y_true = testGen.classes[testGen.index_array]\n",
    "    \n",
    "    plot_confusion_matrix(y_true, y_pred.argmax(axis=-1), labels, out_path=model_out_dir)\n",
    "    class_report = classification_report(y_true, y_pred.argmax(axis=-1), target_names=labels)\n",
    "\n",
    "    with open(model_out_dir + \"/classification_report.txt\", \"w\") as text_file:\n",
    "        text_file.write(\"%s\" % class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "data (InputLayer)            (None, 100, 100, 3)       0         \n",
      "_________________________________________________________________\n",
      "lambda_12 (Lambda)           (None, 100, 100, 4)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 100, 100, 16)      1616      \n",
      "_________________________________________________________________\n",
      "conv1_relu (Activation)      (None, 100, 100, 16)      0         \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling2D)         (None, 50, 50, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 50, 50, 32)        12832     \n",
      "_________________________________________________________________\n",
      "conv2_relu (Activation)      (None, 50, 50, 32)        0         \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling2D)         (None, 25, 25, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 25, 25, 64)        51264     \n",
      "_________________________________________________________________\n",
      "conv3_relu (Activation)      (None, 25, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "pool3 (MaxPooling2D)         (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv4 (Conv2D)               (None, 12, 12, 128)       204928    \n",
      "_________________________________________________________________\n",
      "conv4_relu (Activation)      (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "pool4 (MaxPooling2D)         (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv5 (Conv2D)               (None, 6, 6, 254)         813054    \n",
      "_________________________________________________________________\n",
      "conv5_relu (Activation)      (None, 6, 6, 254)         0         \n",
      "_________________________________________________________________\n",
      "pool5 (MaxPooling2D)         (None, 3, 3, 254)         0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 2286)              0         \n",
      "_________________________________________________________________\n",
      "fcl1 (Dense)                 (None, 1024)              2341888   \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "fcl2 (Dense)                 (None, 500)               512500    \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "fcl3 (Dense)                 (None, 200)               100200    \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 95)                19095     \n",
      "=================================================================\n",
      "Total params: 4,057,377\n",
      "Trainable params: 4,057,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Found 50503 images belonging to 95 classes.\n",
      "Found 8865 images belonging to 95 classes.\n",
      "Found 2559 images belonging to 95 classes.\n",
      "Training directory:\n",
      " /home/miber/data/ingredients-images/ingredients-train\n",
      "Epoch 1/1\n",
      "2021/2021 [==============================] - 80s 40ms/step - loss: 2.2085 - accuracy: 0.4599 - val_loss: 2.9208 - val_accuracy: 0.7657\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.76571, saving model to ingredients_training_output_files/ingredients_detection_model/model.h5\n",
      "355/355 [==============================] - 9s 27ms/step\n",
      "Validation: accuracy = 0.762098  ;  loss_v = 2.640023\n",
      "103/103 [==============================] - 4s 42ms/step\n",
      "Hold out set: accuracy = 2.454285  ;  loss = 0.286049\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 4s 42ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (94, 94), indices imply (95, 95)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[0;34m(blocks, axes)\u001b[0m\n\u001b[1;32m   1677\u001b[0m                 blocks = [\n\u001b[0;32m-> 1678\u001b[0;31m                     \u001b[0mmake_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1679\u001b[0m                 ]\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mmake_block\u001b[0;34m(values, placement, klass, ndim, dtype, fastpath)\u001b[0m\n\u001b[1;32m   3283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3284\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, values, placement, ndim)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;34m\"Wrong number of items passed {val}, placement implies \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                 \u001b[0;34m\"{mgr}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmgr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: Wrong number of items passed 94, placement implies 95",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-905250aedcb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Training and evaluating the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_softmax_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mtrain_and_evaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ingredients_detection_model\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sparse_categorical_crossentropy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-53-070e98b09ee0>\u001b[0m in \u001b[0;36mtrain_and_evaluate_model\u001b[0;34m(model, name, epochs, batch_size, validation_percent, verbose, loss, use_checkpoint)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestGen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtestGen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m     \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_out_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0mclass_report\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-070e98b09ee0>\u001b[0m in \u001b[0;36mplot_confusion_matrix\u001b[0;34m(y_true, y_pred, classes, out_path)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mdf_cm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_cm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquare\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbar_kws\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"shrink\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                 \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;31m# For data is list-like, or Iterable (will consume into list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_ndarray\u001b[0;34m(values, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mblock_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[0;34m(blocks, axes)\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"values\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m         \u001b[0mtot_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m         \u001b[0mconstruction_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtot_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mconstruction_error\u001b[0;34m(tot_items, block_shape, axes, e)\u001b[0m\n\u001b[1;32m   1717\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Empty data passed with indices specified.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m     raise ValueError(\n\u001b[0;32m-> 1719\u001b[0;31m         \u001b[0;34m\"Shape of passed values is {0}, indices imply {1}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpassed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimplied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1720\u001b[0m     )\n\u001b[1;32m   1721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (94, 94), indices imply (95, 95)"
     ]
    }
   ],
   "source": [
    "# Model set up and learning: Base Model (CNN + FF with Softmax Activation)\n",
    "# Based on https://github.com/Horea94/Fruit-Images-Dataset\n",
    "\n",
    "def image_process(x):\n",
    "    '''\n",
    "    Creates a custom layer that converts the original image from \n",
    "    RGB to HSV and grayscale and concatenates the results\n",
    "    '''\n",
    "    import tensorflow as tf # needs to be in here, when loading the model back in\n",
    "    \n",
    "    hsv = tf.image.rgb_to_hsv(x)\n",
    "    gray = tf.image.rgb_to_grayscale(x)\n",
    "    image = tf.concat([hsv, gray], axis=-1)\n",
    "    return image\n",
    "\n",
    "\n",
    "def cnn_softmax_model(input_shape, num_classes):\n",
    "    '''\n",
    "    The Model: \n",
    "    - CNN with relu activation and max pooling, 5 CNN layers\n",
    "    - 3 feed forward layers with dropout with one softmax activation afterward\n",
    "    '''\n",
    "    \n",
    "    img_input = Input(shape=input_shape, name='data')\n",
    "    x = Lambda(image_process)(img_input)\n",
    "    x = Conv2D(16, (5, 5), strides=(1, 1), padding='same', name='conv1')(x)\n",
    "    x = Activation('relu', name='conv1_relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='valid', name='pool1')(x)\n",
    "    x = Conv2D(32, (5, 5), strides=(1, 1), padding='same', name='conv2')(x)\n",
    "    x = Activation('relu', name='conv2_relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='valid', name='pool2')(x)\n",
    "    x = Conv2D(64, (5, 5), strides=(1, 1), padding='same', name='conv3')(x)\n",
    "    x = Activation('relu', name='conv3_relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='valid', name='pool3')(x)\n",
    "    x = Conv2D(128, (5, 5), strides=(1, 1), padding='same', name='conv4')(x)\n",
    "    x = Activation('relu', name='conv4_relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='valid', name='pool4')(x)\n",
    "    x = Conv2D(254, (5, 5), strides=(1, 1), padding='same', name='conv5')(x)\n",
    "    x = Activation('relu', name='conv5_relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='valid', name='pool5')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024, activation='relu', name='fcl1')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(500, activation='relu', name='fcl2')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(200, activation='relu', name='fcl3')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    out = Dense(num_classes, activation='softmax', name='predictions')(x)\n",
    "    model = Model(inputs=img_input, outputs=out)\n",
    "    return model\n",
    "\n",
    "# Training and evaluating the model\n",
    "model = cnn_softmax_model(input_shape=input_shape, num_classes=num_classes)\n",
    "train_and_evaluate_model(model, name=\"ingredients_detection_model\",  loss=\"sparse_categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model set up and learning: With SVM\n",
    "# Based on https://github.com/Horea94/Fruit-Images-Dataset\n",
    "from keras.regularizers import l2\n",
    "\n",
    "def image_process(x):\n",
    "    '''\n",
    "    Creates a custom layer that converts the original image from \n",
    "    RGB to HSV and grayscale and concatenates the results\n",
    "    '''\n",
    "    import tensorflow as tf # needs to be in here, when loading the model back in\n",
    "    \n",
    "    hsv = tf.image.rgb_to_hsv(x)\n",
    "    gray = tf.image.rgb_to_grayscale(x)\n",
    "    image = tf.concat([hsv, gray], axis=-1)\n",
    "    return image\n",
    "\n",
    "\n",
    "def cnn_svm_model(input_shape, num_classes):\n",
    "    '''\n",
    "    The Model: \n",
    "    - CNN with relu activation and max pooling, 6 CNN layers\n",
    "    - 3 feed forward layers with dropout with one softmax activation afterward\n",
    "    '''\n",
    "    \n",
    "    img_input = Input(shape=input_shape, name='data')\n",
    "    x = Lambda(image_process)(img_input)\n",
    "    x = Conv2D(16, (5, 5), strides=(1, 1), padding='same', name='conv1')(x)\n",
    "    x = Activation('relu', name='conv1_relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='valid', name='pool1')(x)\n",
    "    x = Conv2D(32, (5, 5), strides=(1, 1), padding='same', name='conv2')(x)\n",
    "    x = Activation('relu', name='conv2_relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='valid', name='pool2')(x)\n",
    "    x = Conv2D(64, (5, 5), strides=(1, 1), padding='same', name='conv3')(x)\n",
    "    x = Activation('relu', name='conv3_relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='valid', name='pool3')(x)\n",
    "    x = Conv2D(128, (5, 5), strides=(1, 1), padding='same', name='conv4')(x)\n",
    "    x = Activation('relu', name='conv4_relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='valid', name='pool4')(x)\n",
    "    x = Conv2D(254, (5, 5), strides=(1, 1), padding='same', name='conv5')(x)\n",
    "    x = Activation('relu', name='conv5_relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='valid', name='pool5')(x)\n",
    "    x = Conv2D(508, (5, 5), strides=(1, 1), padding='same', name='conv6')(x)\n",
    "    x = Activation('relu', name='conv6_relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='valid', name='pool6')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024, activation='relu', name='fcl1')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(500, activation='relu', name='fcl2')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(200, activation='relu', name='fcl3')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(num_classes, name='svm', W_regularizer=l2(weight))(x)\n",
    "    out = Dense(num_classes, activation='softmax', name='prediction')(x)\n",
    "    model = Model(inputs=img_input, outputs=out)\n",
    "    return model\n",
    "\n",
    "# Training and evaluating the model\n",
    "model = cnn_svm_model(input_shape=input_shape, num_classes=num_classes)\n",
    "train_and_evaluate_model(model, name=\"ingredients_detection_model_hinge\", loss='categorical_hinge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making single image predictions\n",
    "# Loading model\n",
    "import sys\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adadelta\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# load model\n",
    "model = load_model('/home/miber/creative-cooking/ingredients_training_output_files/ingredients_detection_model/model.h5')\n",
    "\n",
    "# Load and resize image\n",
    "image_size = (100, 100)\n",
    "input_shape = (1, 100, 100, 3) \n",
    "\n",
    "#image = '/home/miber/data/ingredients-images/ingredients-train/Apple/0_100.jpg'\n",
    "image = '/home/miber/data/ingredients-images/ingredients-train/Banana/222_100.jpg'\n",
    "image = cv2.imread(image)\n",
    "image = cv2.resize(image,image_size)\n",
    "image = np.reshape(image, input_shape)\n",
    "\n",
    "# Run model\n",
    "learning_rate = 0.1\n",
    "optimizer = Adadelta(lr=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "prediction = model.predict(image)\n",
    "\n",
    "# Format prediction\n",
    "label_file = '/home/miber/creative-cooking/labels.txt'\n",
    "\n",
    "with open(label_file, \"r\") as f:\n",
    "    labels = [x.rstrip('\\n') for x in f.readlines()]\n",
    "\n",
    "print('Prediction Ingredient:')\n",
    "\n",
    "for ingredient, prob in sorted(list(zip(labels, prediction[0])), key = lambda x: -x[1])[:5]:\n",
    "    print('{}: {:.2f}%'.format(ingredient, round(prob, 4) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ./ingredients-prediction.py /home/miber/data/ingredients-images/ingredients-train/Apple/0_100.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Fruits-360 CNN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
